---
title: 系统架构设计师教程(施工中)
date: 2020-12-24 10:08:59
categories: 读书笔记
tags: [读书笔记]
---
# 第一章 计算机组成与体系结构
## 1.1 计算机系统组成
###  1.1.1 计算机硬件的组成  
&emsp;&emsp;计算机系统由`运算器`，`控制器`，`存储器`，`输入设备`，`输出设备`五部分组成。

1. 控制器  
&emsp;&emsp;控制器是分析和执行指令的部件，也是统一指挥并控制计算机各部件协调工作的中心部件，所依据的是机器指令。  
&emsp;&emsp;控制器的组成包含：
    * 程序计数器`PC`：存储P下一条要执行指令的地址。
    * 指令寄存器`IR`：存储即将执行的指令。
    * 指令译码器`ID`：对指令中的操作码字段进行分析解释。
    * 时序部件：提供时序控制信号。
2. 运算器  
&emsp;&emsp;运算器也称为算术逻辑单元（ArithmeticandLogicUnit，ALU），其主要功能是在控制器的控制下完成各种算术运算和逻辑运算。   
&emsp;&emsp;运算器的组成包含：  
    * 算术逻辑单元`ALU`：数据的算术运算和逻辑运算。
    * 累加寄存器`AC`：通用寄存器，为ALU提供一个工作区，用于暂存数据。
    * 数据缓冲寄存器`DR`：写内存时，暂存指令或数据。
    * 状态条件寄存器`PSW`：存状态标志与控制标志（争议点：也有将其归为控制器的）。  
3. 主存储器  
&emsp;&emsp;主存储器也称为内存储器（通常简称为“内存”或“主存”）。存储现场操作的信息与中间结果，包括机器指令和数据。  
4. 辅助存储器  
&emsp;&emsp;辅助存储器也称为外存储器，通常简称为外存或辅存。存储需要长期保存的各种信息。  
5. 输入设备  
&emsp;&emsp;输入设备的任务是把人们编好的程序和原始数据送到计算机中去,并且将它们转换成计算机内部所能识别和接受的信息方式。按输入信息的形态可分为字符（包括汉字）输入、图形输入、图像输入及语音输入等。目前，常见的输入设备有键盘、鼠标、扫描仪等。  
6. 输出设备  
&emsp;&emsp;输出设备的任务是将计算机的处理结果以人或其他设备所能接受的 形式送出计算机。目前，最常用的输出设备是打印机和显示器。有些设备既可以是输入设备，同时也可以是输出设备，例如，辅助存储器、自动控制和检测系统中使用的数模转换装置等。

### 1.1.2 计算机系统结构的分类
#### 1.存储程序的概念
&emsp;&emsp;“存储程序”由冯·诺依曼提出，是计算机的系统架构，符合存储程序概念的计算机统称为冯·诺依曼型计算机。它的基本含义有以下三点：    
* 计算机由运算器、存储器、控制器、输入设备、输出设备5部分组成。
* 计算机内部由二进制来表示指令和数据。
* 将编好的程序和数据事先存入存储器中，然后再启动计算机工作。

#### 2. Flynn分类
&emsp;&emsp;1966年，Michael.J.Flynn提出根据`指令流`、`数据流`的多倍性特征对计算机系统进行分类（通常称为 Flynn 分类法）。  

* 指令流：指机器执行的指令序列
* 数据流：指由指令流调用的数据序列，包括输入数据和中间结果，但不包括输出数据。  

&emsp;&emsp;Flynn根据不同的指令流-数据流组织方式，把计算机系统分成以下四类：

1. 单指令流单数据流（Single Instruction stream and Single Data stream，`SISD`）  
&emsp;&emsp;SISD其实就是传统的顺序执行的单处理器计算机，其指令部件每次只对一条指令进行译码，并只对一个操作部件分配数据。  
2. 单指令流多数据流（Single Instruction stream and Multiple Data stream，`SIMD`）  
&emsp;&emsp;SIMD以并行处理机（矩阵处理机）为代表，并行处理机包括多个重复的处理单元，由单一指令部件控制，按照同一指令流的要求为它们分配各自所需的不同数据。  
3. 多指令流单数据流（Multiple Instruction stream and Single Data stream，`MISD`）  
&emsp;&emsp;MISD具有多个处理单元，按多条不同指令的要求对同一数据流及其中间结果进行不同的处理。一个处理单元的输出又作为另一个处理单元的输入。这类系统实际上很少见到。有文献把流水线看作多个指令部件，称流水线计算机是 MISD。
4. 多指令流多数据流（Multiple Instruction stream and Multiple Data stream，`MIMD`）  
&emsp;&emsp;MIMD是指能实现作业、任务、指令等各级全面并行的多机系统。如多核处理器、多处理机等。

### 1.1.3 复杂指令集系统与精简指令集系统
&emsp;&emsp;一个处理器支持的指令和指令的字节级编码成为其指令集体系结构，有CISC和RISC两种发展途径。

#### 1. CISC
&emsp;&emsp;CISC(Complex Instruction Set Computer)全称是复杂指令系统计算机，它的基本思想是进一步增强其原有指令的功能，用更为复杂的新指令取代原先由软件子程序完成的功能。`目前使用的绝大多数计算机都属于CISC类型`。  

&emsp;&emsp;CSIC的特点如下：

1. 指令数量众多  
&emsp;&emsp;指令系统拥有大量的指令，通常有 100～250 条。  
2. 指令使用频率相差悬殊  
&emsp;&emsp;最常使用的是一些比较简单的指令，仅占指令总数的20%，但在程序中出现的频率却占80%。而大部分复杂指令却很少使用。  
3. 支持多种寻址方式  
&emsp;&emsp;支持的寻址方式通常为 5～20 种。  
4. 变长的指令  
&emsp;&emsp;指令长度不是固定的，变长的指令增加指令译码电路的复杂性。  
5. 指令可以对主存单元中的数据直接进行处理  
&emsp;&emsp;典型的 CISC 通常都有指令能够直接对主存单元中的数据进行处理，`但其执行速度较慢`。  
6. 以微程序控制为主  
&emsp;&emsp;CISC的指令系统很复杂，难以用硬布线逻辑（组合逻辑）电路实现控制器，通常采用微程序控制。

#### 2. RISC
&emsp;&emsp;RISC(Reduced Instruction Set Computer)全称是精简指令系统计算机。它的基本思想是通过减少指令总数和简化指令功能降低硬件设计的复杂度，使指令能单周期运行，并通过优化编译提高指令的执行速度，采用硬布线逻辑优化编译程序。

&emsp;&emsp;RISC的特点如下：

1. 指令数量少  
&emsp;&emsp;优先选取使用频率最高的一些简单指令和一些常用指令，避免使用复杂指令。只提供了LOAD（从存储器中读数）和STORE（把数据写入存储器）两条指令对存储器操作，其余所有的操作都在 CPU 的寄存器之间进行。  
2. 指令的寻址方式少  
&emsp;&emsp;通常只支持寄存器寻址方式、立即数寻址方式和相对寻址方式。  
3. 指令长度固定，指令格式种类少  
&emsp;&emsp;因为 RISC 指令数量少、格式少、相对简单，其指令长度固定，指令之间各字段的划分比较一致，译码相对容易。  
4. 以硬布线逻辑控制为主  
&emsp;&emsp;为了提高操作的执行速度，通常采用硬布线逻辑（组合逻辑）来构建控制器。  
5. 单周期指令执行  
&emsp;&emsp;因为简化了指令系统，很容易利用流水线技术，使得大部分指令都能在一个机器周期内完成。少数指令可能会需要多周期，例如LOAD/STORE 指令因为需要访问存储器，其执行时间就会长一些。  
6. 优化的编译器  
&emsp;&emsp;RISC的精简指令集使编译工作简单化。因为指令长度固定、格式少、寻址方式少，编译时不必在具有相似功能的许多指令中进行选择，也不必为寻址方式的选择而费心，同时易于实现优化，从而可以生成高效率执行的机器代码。
7. CPU中的通用寄存器数量多  
&emsp;&emsp;一般在32个以上，有的可达上千个。

### 1.1.4 总线

&emsp;&emsp;总线是一组能为多个部件分时共享的公共信息传送线路。共享是指总线上可以挂接多个部件，各个部件之间相互交换的信息都可以通过这组公共线路传送；分时是指同一时刻只允许有一个部件向总线发送信息，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突。当然，在同一时刻，允许多个部件同时从总线上接收相同的信息。  
&emsp;&emsp;按总线相对于 CPU 或其他芯片的位置可分为`内部总线`和`外部总线`两种。在 CPU 内部，寄存器之间和算术逻辑部件ALU与控制部件之间传输数据所用的总线称为内部总线；外部总线是指CPU与内存RAM、ROM和输入/输出设备接口之间进行通信的通路。由于CPU通过总线实现程序取指令、内存/外设的数据交换，在 CPU 与外设一定的情况下，`总线速度是制约计算机整体性能的最大因素`。  
&emsp;&emsp;按总线功能来划分，又可分为`地址总线`、`数据总线`、`控制总线`三类，人们通常所说的总线都包括这三个组成部分，地址总线用来传送地址信息，数据总线用来传送数据信息，控制总线用来传送各种控制信号。

## 1.2 存储器系统
 
&emsp;&emsp;传统的存储器系统一般分为`高速缓冲存储器（Cache）`、`主存`、`辅存`三级。主存可由CPU直接访问，存取速度快，但容量较小，一般用来存放当前正在执行的程序和数据。辅存设置在主机外部，它的存储容量大，价格较低，但存取速度较慢，一般用来存放暂时不参与运行的程序和数据，CPU 不可以直接访问辅存，辅存中的程序和数据在需要时才传送到主存，因此它是主存的补充和后援。当 CPU 速度很高时，为了使访问存储器的速度能与 CPU 的速度相匹配，又在主存和 CPU 间增设了一级 Cache。Cache 的存取速度比主存更快，但容量更小，用来存放当前最急需处理的程序和数据，以便快速地向 CPU 提供指令和数据。因此，计算机采用多级存储器体系，确保能够获得尽可能高的存取速率，同时保持较低的成本。  
&emsp;&emsp;存储器中数据常用的存取方式有`顺序存取`、`直接存取`、`随机存取`和`相联存取`四种。
1. 顺序存取  
&emsp;&emsp;存储器的数据以记录的形式进行组织。对数据的访问必须按特定的线性顺序进行。磁带存储器采用顺序存取的方式。
2. 直接存取  
&emsp;&emsp;与顺序存取相似，直接存取也使用一个共享的读写装置对所有的数据进行访问。但是，每个数据块都拥有唯一的地址标识，读写装置可以直接移动到目的数据块所在位置进行访问。存取时间也是可变的。磁盘存储器采用直接存取的方式。
3. 随机存取  
&emsp;&emsp;存储器的每一个可寻址单元都具有自己唯一的地址和读写装置，系统可以在相同的时间内对任意一个存储单元的数据进行访问，而与先前的访问序列无关。主存储器采用随机存取的方式。
4. 相联存取  
&emsp;&emsp;相联存取也是一种随机存取的形式，但是选择某一单元进行读写是取决于其内容而不是其地址。与普通的随机存取方式一样，每个单元都有自己的读写装置，读写时间也是一个常数。使用相联存取方式，可以对所有的存储单元的特定位进行比较，选择符合条件的单元进行访问。为了提高地址映射的速度，Cache 采取相联存取的方式。
    
### 1.2.1 主存储器
&emsp;&emsp;主存用来存放计算机运行期间所需要的程序和数据，CPU可直接随机地进行读/写。主存具有一定容量，存取速度较高。由于CPU要频繁地访问主存，所以主存的性能在很大程度上影响了整个计算机系统的性能。根据工艺和技术不同，主存可分为`随机存取存储器`和`只读存储器`。

#### 1.随机存取存储器
&emsp;&emsp;随机存取存储器（Random Access Memory，RAM）既可以写入也可以读出，但断电后信息无法保存，因此只能用于暂存数据。RAM又可分为DRAM（Dynamic RAM，动态RAM）和SRAM（Static RAM，静态RAM）两种，DRAM 的信息会随时间逐渐消失，因此需要定时对其进行刷新维持信息不丢失；SRAM 在不断电的情况下信息能够一直保持而不会丢失。DRAM 的密度大于 SRAM 且更加便宜，但 SRAM 速度快，电路简单（不需要刷新电路），然而容量小，价格高。
#### 2.只读存储器
&emsp;&emsp;只读存储器（Read Only Memory，ROM）可以看作 RAM 的一种特殊形式，其特点是存储器的内容只能随机读出而不能写入。这类存储器常用来存放那些不需要改变的信息。由于信息一旦写入存储器就固定不变了，即使断电，写入的内容也不会丢失，所以又称为固定存储器。ROM 一般用于存放系统程序 BIOS（Basic Input Output System，基本输入输出系统）。
#### 3.内存编址方法
&emsp;&emsp;在计算机系统中，存储器中每个单元的位数是相同且固定的，称为存储器编址单位。不同的计算机，存储器编址的方式不同，主要有字编址和字节编址。  
&emsp;&emsp;内存一般以字节（8 位）为单位，或者以字为单位（字的长度可大可小，例如 16 位或者 32 位等，在这类试题中，一般会给出字的大小）。
```Example
例如，内存地址从 AC000H 到 C7FFFH
则共有C7FFFFH-AC000H+1=1C000H 个地址单元

转换为10进制则有 
 12*16^3+1*16^4
=12*2^12+1*2^16
=12*2^2KB+1*2^6KB
=112KB

如果该内存地址按字（16bit）编址，则共有 112KB*16 位。
假设该内存由 28 片存储器芯片构成，已知构成此内存的芯片每片有 16KB 个存储单元，则该芯片每个存储单元存储
（112KB*16）/（28*16KB）=4 位。
```

### 1.2.2 辅助存储器
#### 1．磁带存储器
&emsp;&emsp;磁带存储器是一种顺序存取的设备，其特点包括：存取时间较长，但存储容量大，便于携带，价格便宜。磁带应用的场景越来越少，目前主要用于资料的归档保存。
#### 2．硬盘存储器
&emsp;&emsp;在硬盘中，信息分布呈以下层次：`记录面`、`圆柱面`、`磁道`和`扇区`，如图所示。 

[![硬盘信息分布示意图](https://s3.ax1x.com/2020/12/24/rg5IJJ.png)](https://imgchr.com/i/rg5IJJ)

&emsp;&emsp;一台硬盘驱动器中有多个磁盘片，每个盘片有两个记录面，每个记录面对应一个磁头，所以`记录面号就是磁头号`，如图中（a）所示。    
&emsp;&emsp;所有的磁头安装在一个公用的传动设备或支架上，磁头一致地沿盘面径向移动，`单个磁头不能单独地移动`。在记录面上，一条条磁道形成一组同心圆，最外圈的磁道为 0 号，往内则磁道号逐步增加，如图中（b）所示。通常将一条磁道划分为若干个段，每个段称为一个扇区或扇段，每个扇区存放一个定长信息块（例如，512 个字节）。一条磁道划分多少扇区，每个扇区可存放多少字节，一般由操作系统决定。磁道上的扇区编号从 1 开始，不像磁头或柱面编号从 0开始。    
&emsp;&emsp;在一个盘组中，各记录面上相同编号（位置）的各磁道构成一个柱面，如图中（c）所示。若每个磁盘片有 m 个磁道，则该硬盘共有 m 个柱面。引入柱面的概念是为了提高硬盘的存储速度。当主机要存入一个较大的文件时，若一条磁道存不完，就需要存放在几条磁道上。这时，应首先将一个文件尽可能地存放在同一柱面中。如果仍存放不完，再存入相邻的柱面内。  
&emsp;&emsp;在磁盘上进行信息的读写时，首先需要定位到目标磁道，这个过程称之为`寻道`，寻道所消耗的时间称为`寻道时间`。定位到目标磁道后，需要定位到目标扇区，此过程通过旋转盘片完成，平均旋转`半圈`可到目标位置。故磁盘访问时间为：`磁盘访问时间（存取时间） = 寻道时间+旋转延迟时间`

### 1.2.3 Cache存储器
&emsp;&emsp;Cache 通常采用相联存储器（ContentAddressable Memory，CAM）。CAM 是一种基于数据内容进行访问的存储设备。当对其写入数据时，CAM 能够自动选择一个未用的空单元进行存储；当要读出数据时，不是给出其存储单元的地址，而是直接给出该数据或者该数据的一部分内容，CAM 对所有存储单元中的数据同时进行比较，并标记符合条件的所有数据以供读取。由于比较是同时、并行进行的，所以，这种基于数据内容进行读写的机制，其速度比基于地址进行读写的方式要快很多。
#### 1．Cache基本原理
&emsp;&emsp;使用 Cache 改善系统性能的依据是程序的`局部性原理`。  
&emsp;&emsp;局部性原理是指程序在执行时呈现出局部性规律，即在一较短的时间内，程序的执行仅局限于某个部分。相应地，它所访问的存储空间也仅局限于某个区域。  
&emsp;&emsp;根据程序的局部性原理，可以把目前常用或将要用到的信息预先放在 Cache 中。当CPU 需要读取数据时，首先在 Cache 中查找是否有所需内容，如果有，则直接从 Cache 中读取；若没有，再从内存中读取该数据，然后同时送往 CPU 和 Cache。如果 CPU 需要访问的内容大多都能在 Cache 中找到（称为`访问命中`），则可以大大提高系统性能。  
&emsp;&emsp;如果以 h 代表对 Cache 的访问命中率（“1-h”称为失效率，或者称为未命中率），t1 表示 cache 的周期时间，t2 表示内存的周期时间，以读操作为例，使用“Cache+主存储器”的系统的平均周期为 t3。则：

$$t3=t1*h+t2*(1-h)$$

&emsp;&emsp;系统的平均存储周期与命中率有很密切的关系，命中率的提高即使很小也能导致性能上的较大改善。

```Example
设某计算机主存的读/写时间为 l00ns，有一个指令和数据合一的Cache
已知该Cache的读/写时间为 10ns，取指令的命中率为 98%，取数的命中率为 95%
在执行某类程序时，约有 1/5 指令需要存/取一个操作数。
假设指令流水线在任何时候都不阻塞，则设置 Cache 后，每条指令的平均访存时间约为：
(2%*100ns+98%*10ns)+1/5*(5%*100ns+95%*10ns)=14.7ns
```

#### 2. 映射机制
&emsp;&emsp;当 CPU 发出访存请求后，存储器地址先被送到 Cache 控制器以确定所需数据是否已在 Cache 中，若命中则直接对 Cache 进行访问。这个过程称为 Cache 的地址映射（映像）。  
&emsp;&emsp;在 Cache 的地址映射中，主存和 Cache 将均分成容量相同的块（页）。常见的映射方法有`直接映射`、`全相联映射`和`组相联映射`。

* 直接映射  

&emsp;&emsp;直接映射以随机存取存储器作为 Cache 存储器，硬件电路较简单。在进行映射时，主存地址被分成三个部分，从高到低依次为：区号、页号以及页内地址，如图所示。  

[![直接映射方式的主存地址](https://s3.ax1x.com/2020/12/24/rgq5Bn.png)](https://imgchr.com/i/rgq5Bn)

&emsp;&emsp;在本例中，内存容量为 1GB，Cache 容量为 8MB，页面的大小为 512KB。直接映射中先分区，再分页。一个区的大小就是 Cache 容量的大小，所以一共分：1GB/8MB=128 个区，区号7位。每个区分：8MB/512KB=16个页，所以页号为 4 位。  
&emsp;&emsp;在直接映射方式中，每个主存页只能复制到某一固定的 Cache 页中，如下图所示。直接映射方式的映射规律是：主存中每个区的第 0 页，只能进入到 Cache 的第 0 页。即：若当前时刻 Cache 中 0 号页已被占据，而 1-15 号页空闲，现在要将 1 区第 0 页（即内存的 16 页）调入 Cache 是会发生冲突的。所以直接映射的块冲突率非常高。

[![直接映射方式](https://s3.ax1x.com/2020/12/24/rgLwCT.png)](https://imgchr.com/i/rgLwCT)

&emsp;&emsp;由于每个区的 N 号页，都必须进入到 Cache 的 N 号页，所以只需要记录区号即可。

* 全相联映射

&emsp;&emsp;全相联映射使用相联存储器组成Cache存储器。在全相联映射方式中，主存的每一页可以映射到Cache的任一页。如果淘汰Cache中某一页的内容，则可调入任一主存页的内容，因而较直接映射方式灵活。  
&emsp;&emsp;在全相联映射方式中，主存地址分为两个部分，分别为地址部分（主存页标记）和数据部分（页内地址）。数据部分用于存放数据，而地址部分则存放该数据的存储器地址。如图所示。

[![主存储示例](https://s3.ax1x.com/2020/12/24/rgjuU1.png)](https://imgchr.com/i/rgjuU1)

&emsp;&emsp;当进行映射时，在我们给定的例子中，当程序访存时，则高 11 位给出主存页号，低19 位给出页内地址。因为每个 Cache 页可映射到 2048 个主存页中的任一页，所以每页的Cache 标记也需要 11 位，以表明它现在所映射的主存页号。因此，Cache 标记信息位数增加，比较逻辑成本随之增加。  
&emsp;&emsp;在全相联映射方式中，主存地址不能直接提取 Cache 页号，而是需要将主存页标记与Cache 各页的标记逐个比较，直到找到标记符合的页（访问 Cache 命中），或者全部比较完后仍无符合的标记（访问 Cache 失败）。因此`这种映射方式速度很慢，失掉了高速缓存的作用，这是全相联映射方式的最大缺点`。如果让主存页标记与各 Cache 标记同时比较，则成本又太高。全相联映射方式因比较器电路难于设计和实现，只适用于小容量 Cache。  

* 组相联映射

&emsp;&emsp;组相联映射（页组映射）介于直接映射和全相联映射之间，是这两种映射的一种折衷方案。全相联映射方式以页为单位，可自由映射，没有固定的对应关系。直接映射方式中，主存分组，主存组内的各页与 Cache 的页之间采取的是固定的映射关系，但各组均可映射到Cache 中。在组相联映射方式中，主存与 Cache 都分组，主存中一个组内的页数与 Cache 的分组数相同，如图所示。

[![组相连映射方式](https://s3.ax1x.com/2020/12/24/rgjLI1.png)](https://imgchr.com/i/rgjLI1)

&emsp;&emsp;在上图给出的例子中，主存分 128 个区，每个区 8 个组，每个组 2 个页。组相联映射方式的主存地址组织如下图所示。

[![组相联映射主存地址示例](https://s3.ax1x.com/2020/12/24/rgvGWV.png)](https://imgchr.com/i/rgvGWV)

&emsp;&emsp;组相联映射的规则是：主存中的组与 Cache 的组形成直接映射关系，而每个组内的页是全相联映射关系。如主存 1 区 0 页，他在 0 组中，所以只能进入 Cache 的 0 组中，至于进入到 Cache 的 0 组 0 页，还是 0 组 1 页，并无强制要求，可任意放置。  
&emsp;&emsp;在组相联映射中，Cache 中每一页的标记位长度为 8 位，因为此时除了要记录区号，还得记录组号，即区号 7 位加组号 1 位等于 8 位。  
&emsp;&emsp;在组相联映射中，由于 Cache 中每组有若干可供选择的页，因而它在映射定位方面较直接映射方式灵活；每组页数有限，因此付出的代价不是很大，可以根据设计目标选择组内页数。

#### 3. 替换算法

&emsp;&emsp;当 Cache 产生了一次访问未命中之后，相应的数据应同时读入 CPU 和 Cache。但是当Cache 已存满数据后，新数据必须替换（淘汰）Cache 中的某些旧数据。最常用的替换算法有以下三种：
1. 随机算法  
&emsp;&emsp;这是最简单的替换算法。随机法完全不管 Cache 块过去、现在及将来的使用情况，简单地根据一个随机数，选择一块替换掉。  
2. 先进先出（First In and First Out，FIFO）算法    
&emsp;&emsp;按调入 Cache 的先后决定淘汰的顺序，即在需要更新时，将最先进入 Cache 的块作为被替换的块。这种方法要求为每块做一记录，记下它们进入 Cache 的先后次序。这种方法容易实现，而且系统开销小。其缺点是`可能会把一些需要经常使用的程序块（如循环程序）替换掉`。  
3. 近期最少使用（Least Recently Used，LRU）算法   
&emsp;&emsp;LRU 算法是把 CPU 近期最少使用的块作为被替换的块。这种替换方法需要随时记录 Cache 中各块的使用情况，以便确定哪个块是近期最少使用的块。LRU 算法相对合理，但实现起来比较复杂，系统开销较大。通常需要对每一块设置一个称为“年龄计数器”的硬件或软件计数器，用以记录其被使用的情况。  

#### 4．写操作
&emsp;&emsp;因为需要保证缓存在 Cache 中的数据与内存中的内容一致，相对读操作而言，Cache 的写操作比较复杂，常用的有以下几种方法。  
1. 写直达（write through）  
&emsp;&emsp;当要写 Cache 时，数据同时写回内存，有时也称为写通。当某一块需要替换时，也不必把这一块写回到主存中去，新调入的块可以立即把这一块覆盖掉。这种方法实现简单，而且能随时保持主存数据的正确性，但`可能增加多次不必要的主存写入`，会降低存取速度。  
2. 写回（write back）  
&emsp;&emsp;CPU 修改 Cache 的某一块后，相应的数据并不立即写入内存单元，而是当该块从 Cache 中被淘汰时，才把数据写回到内存中。在采用这种更新策略的Cache块表中，一般有一个标志位，当一块中的任何一个单元被修改时，标志位被置“1”。    
&emsp;&emsp;在需要替换掉这一块时，如果标志位为“1”，则必须先把这一块写回到主存中去之后，才能再调入新的块；如果标志位为“0”，则这一块不必写回主存，只要用新调入的块覆盖掉这一块即可。这种方法的优点是`操作速度快`，缺点是`因主存中的字块未随时修改而有可能出错`。  
3. 标记法  
&emsp;&emsp;对 Cache 中的每一个数据设置一个有效位。当数据进入 Cache 后，有效位置“1”；而当 CPU 要对该数据进行修改时，数据只需写入内存并同时将该有效位置“0”。当要从 Cache 中读取数据时需要测试其有效位，若为“l”则直接从 Cache 中取数，否则，从内存中取数。  

## 1.3 流水线
&emsp;&emsp;流水线技术把一个任务分解为若干顺序执行的子任务，不同的子任务由不同的执行机构负责执行，而这些机构可以`同时并行工作`。在任一时刻，任一任务只占用其中一个执行机构，这样就可以实现多个任务的重叠执行，以提高工作效率。  
### 1.3.1 流水线周期
&emsp;&emsp;流水线应用过程中，会将需要处理的工作分为 N 个阶段，`最耗时的那一段所消耗的时间为流水线周期`。如：使用流水线技术执行 100 条指令，每条指令取指 2ms，分析 4ms，执行 1ms，则流水线周期为 4ms。  
### 1.3.2 计算流水线执行时间
&emsp;&emsp;延续上面的场景，将 1 个任务的执行过程可分成 N 个阶段，假设每个阶段完成时间为 t，则完成该任务所需的时间即为 Nt。若以传统的方式，则完成 k 个任务所需的时间是kNt；而使用流水线技术执行，且花费的时间是 Nt+(k-1)t。也就是说，除了第 1 个任务需要完整的时间外，其他都通过并行，节省下了大量的时间。所以流水线的执行时间可通俗的表达为：  
>流水线执行时间=第 1 条指令的执行时间+（n-1）\*流水线周期   
注：n 代表需要处理的任务数量。

```
    在考试时，又需要特别注意一个细节问题。
    流水线的执行时间计算，其实进一步可以分理论情况与实践情况两种不同的处理方式。
例：
    某计算机系统，一条指令的执行需要经历取指（2ms）、分析（4ms）、执行（1ms）三个阶段，现要执行 100 条指令，利用流水线技术需要多长时间?
    理论上来说，1 条指令的执行时间为：2ms+4ms+1ms=7ms。
    所以：理论流水线执行时间=2ms+4ms+1ms+(100-1)*4=403ms。
    而实际上，真正做流水线处理时，考虑到处理的复杂性，会将指令的每个执行阶段的时间都统一为流水线周期，即 1 条指令的执行时间为：4ms+4ms+4ms=12ms。 
    所以：实际流水线执行时间=4ms+4ms+4ms+(100-1)*4=408ms。

    考试时 80%以上的概率采用理论公式计算，所以考试时需要以理论公式计算，若计算的结果无正确选项才考虑采用实际公式计算。
```
### 1.3.3 流水线的吞吐率
&emsp;&emsp;流水线的吞吐率（Though Put rate，TP）是指在`单位时间`内`流水线所完成的任务数量或输出的结果数量`。有些文献也称为平均吞吐率、实际吞吐率。  
&emsp;&emsp;计算流水线吞吐率的最基本的公式如下：

$$ 
TP=\frac {n} {T_k} 
$$

&emsp;&emsp;其中n为任务数量，$T_k$是处理完成n个任务所用的时间  

&emsp;&emsp;流水线的最大吞吐率为：

$$TP_{max}=\lim_{n \to \infty} \frac n {(k+n-1)\Delta t}=\frac 1 {\Delta t} $$

### 1.3.4 流水线的加速比
&emsp;&emsp;在流水线中，因为在同一时刻，有多个任务在重叠地执行，虽然完成一个任务的时间与单独执行该任务相近（甚至由于分段的缘故，可能更多一些），但是从整体上看完成多个任务所需的时间则大大减少。   
&emsp;&emsp;完成同样一批任务，`不使用流水线所用的时间`与`使用流水线所用的时间`之比称为流水线的加速比（speedup ratio）。如果不使用流水线，即顺序执行所用的时间为 T0 ，使用流水线的执行时间为 Tk ，则计算流水线加速比的基本公式如下：

$$ S=\frac {T_0} {T_k} $$

&emsp;&emsp;如果流水线各个流水段的执行时间都相等（设为Δt），则一条 k 段流水线完成 n 个连续任务所需要的时间为(k+n-1)Δt。如果不使用流水线，即顺序执行这 n 个任务，则所需要的时间为 nkΔt。  
&emsp;&emsp;因此，各个流水段执行时间均相等的一条 k 段流水线完成 n 个连续任务时的实际加速比为：

$$ S=\frac {nk\Delta t} {(k+n-1)\Delta t}=\frac {nk} {k+n-1}$$

&emsp;&emsp;这种情况下的最大加速比为：

$$ S_{max}=\lim_{n \to \infty} \frac {nk} {k+n-1}=k$$

# 第二章 操作系统
## 2.1 操作系统的类型与结构
### 2.1.1 操作系统的定义
&emsp;&emsp;操作系统（Operating System，OS）是计算机系统中的核心系统软件，负责管理和控制计算机系统中的硬件和软件资源，合理地组织计算机工作流程和有效地利用资源，在计算机与用户之间起接口的作用。

### 2.1.2 操作系统的分类
&emsp;&emsp;按照操作系统的功能划分，操作系统的基本类型主要有
* 批处理操作系统
* 分时操作系统
* 实时操作系统
* 网络操作系统
* 分布式操作系统
* 嵌入式操作系统
* 微内核操作系统

## 2.2 操作系统基本原理
### 2.2.1 进程管理
#### 1. 进程的概念
&emsp;&emsp;在多道程序处理系统中有多个并发执行的程序，为了描述系统中程序执行时动态变化的过程引入了`进程`。进程是资源分配和独立运行的基本单位。
#### 2．进程的状态转换
&emsp;&emsp;由进程运行的间断性，决定了进程至少具有以下三种状态：
1. 就绪状态  
&emsp;&emsp;当进程已分配了除 CPU 以外的所有必要的资源后，只要能再获得处理机，便能立即执行，把这时的进程状态称为`就绪状态`。在一个系统中，可以有多个进程同时处于就绪状态，通常把它们排成一个队列，称为`就绪队列`。
2. 执行状态  
&emsp;&emsp;指进程已获得处理机，其程序正在执行。在单处理机系统中，只能有一个进程处于执行状态。
3. 阻塞状态  
&emsp;&emsp;指进程因发生某事件（如请求 I/O、申请缓冲空间等）而暂停执行时的状态，亦即进程的执行受到阻塞，故称这种暂停状态为`阻塞状态`，有时也称为`等待`状态或`睡眠`状态。通常将处于阻塞状态的进程排成一个队列，称为`阻塞队列`。

&emsp;&emsp;进程的状态随着自身的推进和外界的变化而变化。例如，就绪状态的进程被进程调度程序选中进入执行状态；执行状态的进程因等待某一事件的发生转入等待状态；等待状态的进程所等待事件来到便进入就绪状态。进程的状态可以动态地相互转换，但阻塞状态的进程不能直接进入执行状态，就绪状态的进程不能直接进入阻塞状态。在任何时刻，任何进程都处于且只能处于这其中一种状态。进程状态的变化情况如下：
1. 运行态→等待态  
&emsp;&emsp;一个进程运行中启动了外围设备，它就变成等待外围设备传输信息的状态。  
&emsp;&emsp;进程在运行中申请资源（主存储空间及外围设备因得不到满足）时，变成等待资源状态。  
&emsp;&emsp;进程在运行中出现了故障（程序出错或主存储器读写错误等），变成等待干预状态。  
2. 等待态→就绪态  
&emsp;&emsp;外围设备工作结束后等待外围设备传输信息的进程结束等待。  
&emsp;&emsp;等待的资源能得到满足时（另一个进程归还了资源），则等待资源者就结束等待。  
&emsp;&emsp;故障排队后让等待干预的进程结束等待。  
&emsp;&emsp;任何一个结束等待的进程必须先变成就绪状态，待分配到处理器后才能运行。  
3. 运行态→就绪态  
&emsp;&emsp;进程用完了一个使用处理器的时间后强迫该进程暂时让出处理器。  
&emsp;&emsp;当有更优先权的进程要运行时也迫使正在运行的进程让出处理器。     
&emsp;&emsp;由于自身或外界原因成为等待状态的进程让出处理器时，它的状态就变成就绪状态。  
4. 就绪态→运行态  
&emsp;&emsp;等待分配处理器的进程，系统按一种选定的策略从处于就绪状态的进程中选择一个进程，让它占用处理器，那个被选中的进程就变成了运行态。

&emsp;&emsp;可用状态转换图表示如下：
[![进程状态转换图](https://s3.ax1x.com/2020/12/28/roqSZ8.png)](https://imgchr.com/i/roqSZ8)


#### 3．挂起状态
&emsp;&emsp;在一些系统中，增加了一些新的进程状态，其中最重要的是挂起状态。引入挂起状态的原因有：
1. 对换的需要  
&emsp;&emsp;为了缓和内存紧张的情况，而将内存中处于阻塞状态的进程换至外存上，使进程又处于一种有别于阻塞状态的新状态。因为即使该进程所期待的事件发生，该进程仍不具备执行条件而不能进入就绪队列，称这种状态为挂起状态。
2. 终端用户的请求  
&emsp;&emsp;当终端用户在自己的程序运行期间，发现有可疑问题时，往往希望使自己的进程暂停下来。也就是说，使正在执行的进程暂停执行，若是就绪进程，则不接受调度以便研究其执行情况或对程序进行修改。把这种静止状态也称为挂起状态。  
3. 父进程请求  
&emsp;&emsp;父进程常希望挂起自己的子进程，以便考查和修改子进程，或者协调各子进程间的活动。
4. 负荷调节的需要  
&emsp;&emsp;当实时系统中的工作负荷较重，有可能影响到对实时任务的控制时，可由系统把一些不重要的进程挂起，以保证系统正常运行。
5. 操作系统的需要  
&emsp;&emsp;操作系统希望挂起某些进程，以便检查运行中资源的使用情况及进行记账。

&emsp;&emsp;综上所述，不难了解挂起状态具有以下三个属性。  
1. 被挂起的进程，原来可能处于就绪状态，此时进程（被挂起）的状态称为挂起就绪；若被挂起的进程原来处于阻塞状态，此时的状态称为挂起阻塞。不论哪种状态，该进程都是不可能被调度而执行的。
2. 处于挂起阻塞状态的进程，其阻塞条件与挂起条件无关；当进程所期待的事件出现后，进程虽不再被阻塞，但仍不能运行，这时，应将该进程从`静止阻塞`状态转换为`挂起就绪`状态。
3. 进程可以由其自身挂起，也可由用户或操作系统等将之挂起。其目的都在于阻止进程继续运行，被挂起的进程只能用显式方式来激活，以便从挂起状态中解脱出来。

&emsp;&emsp;下图所示为具有挂起操作的进程状态的演变情况：

[![带有挂起操作的进程状态演变图](https://s3.ax1x.com/2020/12/28/roL4AO.png)](https://imgchr.com/i/roL4AO)

#### 4．进程互斥与同步
&emsp;&emsp;进程互斥：一组并发进程中一个或多个程序段，因共享某一共有资源而导致必须以一个不允许交叉执行的单位执行。也就是说互斥是要保证临界资源在某一时刻只被一个进程访问。`（竞争关系）`  
&emsp;&emsp;进程同步：把异步环境下的一组并发进程因直接制约而互相发送消息而进行互相合作、互相等待，使得各进程按一定的速度执行的过程称为进程同步。也就是说进程之间是异步执行的，同步即是使各进程按一定的制约顺序和速度执行。`（协作关系）`

&emsp;&emsp;进程在并发执行中可以共享系统中的资源。但是临界资源的访问则必须互斥进行，必须有专门的同步机构来协调它们，协调准则如下：
1. 空闲让进  
&emsp;&emsp;无进程处于临界区时，若有进程要求进入临界区则立即允许其进入。
2. 忙则等待  
&emsp;&emsp;当已有进程进入其临界区时，其他试图进入各自临界区的进程必须等待，以保证诸进程互斥地进入临界区。
3. 有限等待  
&emsp;&emsp;有若干进程要求进入临界区时，应在有限时间内使一进程进入临界区，即它们不应相互等待而谁也不进入临界区。
4. 让权等待  
&emsp;&emsp;对于等待进入临界区的进程必须释放其占有的 CPU。

&emsp;&emsp;信号量可以有效地实现进程的同步和互斥。在操作系统中，信号量是一个整数。当信号量大于等于 0 时，代表可供并发进程使用的资源实体数，当信号量小于零时则表示正在等待使用临界区的进程数。建立一个信号量必须说明所建信号量代表的意义和设置初值，以及建立相应的数据结构，以便指向那些等待使用该临界区的进程。  
&emsp;&emsp;对信号量只能施加特殊的操作：P 操作和 V 操作。P 操作和 V 操作都是不可分割的原子操作，也称为原语。因此，P 原语和 V 原语执行期间不允许中断发生。  
&emsp;&emsp;P（sem）操作的过程是将信号量 sem 值减 l，若 sem 的值成负数，则调用 P 操作的进程暂停执行，直到另一个进程对同一信号量做 V 操作。V（sem）操作的过程是将信号量sem 值加 1，若 sem 的值小于等于 0，则从相应队列（与 sem 有关的队列）中选一个进程并唤醒它。

&emsp;&emsp;一般 P 操作与 V 操作的定义如下所述。
```
P 操作：
P（sem）{
    sem = sem - 1;
    if（sem < 0）
        进程进入等待状态；
    else 
        继续进行；
    } 
V 操作：
V（sem）{
    sem = sem + 1;
    if（sem ≤ 0）
        唤醒队列中的一个等待进程；
    else 
        继续进行；
    }
```
&emsp;&emsp;要用 P，V 操作实现进程同步，需要引进私用信号量。私用信号量只与制约进程和被制约进程有关，而不是与整组并发进程相关。与此相对，进程互斥使用的信号量为公用信号量。首先为各并发进程设置私用信号量，然后为私用信号量赋初值，最后利用 P，V 原语和私用信号量规定各进程的执行顺序。

#### 5. 前趋图
&emsp;&emsp;前趋图是一个由结点和有向边构成的有向无循环图。该图通常用于表现事务之间先后顺序的制约关系。图中的每个结点可以表示一个语句、一个程序段或是一个进程，结点间的有向边表示两个结点之间存在的前趋关系。

&emsp;&emsp;例如在计算机中，经常采用流水线方式执行指令，每一条指令都可以分解为取指、分析和执行三步。取指操作为 Ai，分析操作为 Bi 和执行操作为 Ci(i=1,2,3)。如下图所示为三个任务各程序段并发执行的前驱图。

[![前趋图示例](https://s3.ax1x.com/2020/12/28/rozRud.png)](https://imgchr.com/i/rozRud)

&emsp;&emsp;图中 A1 没有前趋结点，称为开始结点，它不受任何制约，可以直接执行；而 B1 与 A2只能在 A1 执行完成之后才能开始，而 B2 必须在 B1 与 A2 完成之后才能开始；C3 没有后继结点，称为终止结点。

&emsp;&emsp;在前趋图中，执行先后顺序的制约关系可分为两种：`直接制约`和`间接制约`。  
&emsp;&emsp;`直接制约`通常是指一个操作中，多个步骤之间的制约关系，也可以说是“同步的进程之间的制约关系”。如上图中的A1、B1、C1 是一条指令的取指、分析、执行的三个步骤，所以它们之间的关系是直接制约。  
&emsp;&emsp;`间接制约`通常是指多个操作之间相同步骤的制约关系，也可以说是“互斥的进程之间的制约关系”。如上图中的A1、A2、A3 之间就存在间接制约的关系。

#### 6．进程调度与死锁
&emsp;&emsp;进程调度即处理器调度（又称上下文转换），它的主要功能是确定在什么时候分配处理器，并确定分给哪一个进程，即让正在执行的进程改变状态并转入就绪队列的队尾，再由调度原语将就绪队列的队首进程取出，投入执行。

&emsp;&emsp;引起进程调度的原因有以下几类：
1. 正在执行的进程执行完毕。
2. 执行中的进程自己调用阻塞原语将自己阻塞起来进入睡眠状态。
3. 执行中的进程调用了 P 原语操作，从而因资源不足而阻塞；或调用 V 原语操作激活了等待资源的进程队列。
4. 在分时系统中，当一进程用完一个时间片。
5. 就绪队列中某进程的优先级变得高于当前执行进程的优先级，也将引起进程调度。

&emsp;&emsp;进程调度的方式有两类：`剥夺方式`与`非剥夺方式`。   
&emsp;&emsp;所谓非剥夺方式是指，一旦某个作业或进程占用了处理器，别的进程就不能把处理器从这个进程手中夺走，直到该进程自己因调用原语操作而进入阻塞状态，或时间片用完而让出处理机。  
&emsp;&emsp;剥夺方式是指，当就绪队列中有进程的优先级高于当前执行进程的优先级时，便立即发生进程调度，转让处理机。

&emsp;&emsp;进程调度的算法是服务于系统目标的策略，对于不同的系统与系统目标，常采用不同的调度算法：
1. 先来先服务（First Come and First Serverd，FCFS）调度算法  
&emsp;&emsp;又称先进先出（First Inand First Out，FIFO）。就绪队列按先来后到原则排队。
2. 优先数调度  
&emsp;&emsp;优先数反映了进程优先级，就绪队列按优先数排队。有两种确定优先级的方法，即`静态优先级`和`动态优先级`。静态优先级是指进程的优先级在进程开始执行前确定，执行过程中不变，而动态优先级则可以在进程执行过程中改变。
3. 轮转法（Round Robin）  
&emsp;&emsp;就绪队列按 FCFS 方式排队。每个进程执行一次占有处理器时间都不超过规定的时间单位（时间片）若超过，则自行释放自己所占有的 CPU 而排到就绪队列的末尾，等待下一次调度。同时，进程调度程序又去调度当前就绪队列中的第一个进程。

&emsp;&emsp;进程管理是操作系统的核心，在进程管理的实现中，如果设计不当，会出现一种尴尬的局面——死锁。当若干个进程互相竞争对方已占有的资源，无限期地等待，不能向前推进时会造成“死锁”。  
&emsp;&emsp;例如，P1 进程占有资源 R1，P2 进程占有资源 R2，这时，P1 又需要资源 R2，P2 也需要资源 R1，它们在等待对方占有的资源时，又不会释放自己占有的资源，因而使双方都进入了无限等待状态。  
&emsp;&emsp;死锁是系统的一种出错状态，它不仅会浪费大量的系统资源，甚至还会导致整个系统的崩溃，所以死锁是应该尽量预防和避免的。

1. 死锁条件   
&emsp;&emsp;产生死锁的主要原因是供共享的系统资源不足，资源分配策略和进程的推进顺序不当。系统资源既可能是可重复使用的永久性资源，也可能是消耗性的临时资源。  
&emsp;&emsp;产生死锁的必要条件是：`互斥条件`、`保持和等待条件`、`不剥夺条件`和`环路等待条件`。  
2. 解决死锁的策略   
&emsp;&emsp;处于死锁状态的进程不能继续执行但又占用了系统资源，从而阻碍其他作业的执行。   
&emsp;&emsp;解决死锁有两种策略：一种是在`死锁发生前采用的预防和避免策略`；另一种是`在死锁发生后采用的检测与恢复策略`。  
&emsp;&emsp;死锁的预防主要是通过打破死锁产生的 4 个必要条件之一来保证不会产生死锁。采用的死锁预防策略通常有资源的`静态分配法`或`有序分配法`，它们分别打破了资源动态分配条件和循环等待条件，因此不会发生死锁。但这样做会大大降低系统资源的利用率和进程之间的并行程度。  
&emsp;&emsp;死锁避免策略，则是在系统进行资源分配时，先执行一个死锁避免算法（典型的如银行家算法），以保证本次分配不会导致死锁发生。由于资源分配很频繁，因此死锁避免策略要耗费大量的 CPU 和时间。

&emsp;&emsp;实际上，系统出现死锁的概率很小，故从系统所花的代价上看，采用死锁发生后的检测与恢复策略要比采用死锁发生前的预防与避免策略代价小一些。

### 2.2.2 存储管理
&emsp;&emsp;存储管理主要是指对内存储器的管理，负责对内存的分配和回收、内存的保护和内存的扩充。存储管理的目的是尽量提高内存的使用效率。存储管理的机制经历了多次变迁，由以前的`单一连续区管理`到`分区存储管理`再发展为`段页式管理`。目前前两种技术已逐步被淘汰。

#### 1. 页式存储管理
&emsp;&emsp;分页的基本思想是把程序的逻辑空间和内存的物理空间按照同样的大小划分成若干页面，并以页面为单位进行分配。在页式存储管理中，系统中虚地址是一个有序对（页号，位移）。系统为每一个进程建立一个页表，其内容包括进程的逻辑页号与物理页号的对应关系、状态等。  
&emsp;&emsp;页式系统的动态地址转换是这样进行的：当进程运行时，其页表的首地址已在系统的动态地址转换机构中的基本地址寄存器中。执行的指令访问虚存地址（p，d）时，首先根据页号 p 查页表，由状态可知，这个页是否已经调入内存。若已调入内存，则得到该页的内存位置 p'，然后，与页内相对位移 d 组合，得到物理地址 r。如果该页尚未调入内存，则产生缺页中断，以装入所需的页，该过程如下图所示。

[![页式存储动态地址转换](https://s3.ax1x.com/2020/12/28/rTEYIH.png)](https://imgchr.com/i/rTEYIH)

&emsp;&emsp;页式虚拟存储管理是在页式存储管理的基础上实现虚拟存储器的。首先把作业信息作为副本存放在磁盘上，作业执行时，把作业信息的部分页面装入内存储器，作业执行时若所访问的页面已在内存中，则按页式存储管理方式进行地址转换，得到欲访问的内存绝对地址，若欲访问的页面不在内存中，则产生一个“缺页中断”，由操作系统把当前所需的页面装入内存储器中。  
&emsp;&emsp;为此，在装入作业时，就应在该作业的页表中指出哪些页已在内存储器中，哪些页还没有装入内存。可用一个标志位指示对应页是否在内存储器，可假设标志位为 1 表示该页在内存，而标志位为 0 表示该页尚未装入内存。为了能方便地从磁盘上找到作业信息的副本，故在页表中还可指出每一页副本在磁盘上的位置。

&emsp;&emsp;当内存中无空闲块时，为了装入一个页面而必须按某种算法从已在内存的页中选择一页，将它暂时调出内存，让出内存空间以存放所需装入的页面，这个工作称为“页面调度”。如何选择调出的页面是很重要的，如果采用了一个不合适的算法，就会出现这样的现象：刚被调出的页面又立即要用，因而又要把它装入，而装入不久又被选中调出，调出不久又被装入，如此反复，使调度非常频繁。这种现象称为`“抖动”`。一个好的调度算法应减少或避免抖动现象。  

&emsp;&emsp;常用的页面调度算法有：
1. 最优（OPT）算法  
&emsp;&emsp;选择不再使用或最远的将来才被使用的页，这是理想的算法，但是难以实现，常用于淘汰算法的比较。  
2. 随机（RAND）算法  
&emsp;&emsp;随机地选择被淘汰的页，开销小，但是可能选中立即就要访问的页。
3. 先进先出算法  
&emsp;&emsp;选择在内存驻留时间最长的页似乎合理，但可能淘汰掉频繁使用的页。另外，使用 FIFO 算法时，在未给予进程分配足够的页面数时，有时会出现给予进程的页面数增多，缺页次数反而增加的异常现象。  
&emsp;&emsp;FIFO 算法简单，易实现。可以把装入内存储器的那些页的页号按进入的先后顺序排成队列，每次总是调出队首的页，当装入一个新页后，把新页的页号排到队尾。
4. 最近最少使用（Least Recently Used，LRU）算法  
&emsp;&emsp;选择离当前时间最近的一段时间内使用得最少的页。这个算法的主要出发点是，如果某个页被访问了，则它可能马上就要被访问；反之，如果某个页长时间未被访问，则它在最近一段时间也不会被访问。

#### 2．段式存储管理
&emsp;&emsp;段式存储管理与页式存储管理相似。分段的基本思想是把用户作业按逻辑意义上有完整意义的段来划分，并以段为单位作为内外存交换的空间尺度。  
&emsp;&emsp;一个作业是由若干个具有逻辑意义的段（如主程序、子程序、数据段等）组成。分段系统中，容许程序（作业）占据内存中许多分离的分区。每个分区存储一个程序分段。这样每个作业需要几对界限地址寄存器，判定访问地址是否越界也就更困难了。在分段存储系统中常常利用存储保护键实现存储保护。分段系统中虚地址是一个有序对（段号，位移）。系统为每个作业建立一个段表，其内容包括段号、段长、内存起始地址和状态等。状态指出这个段是否已调入内存，即内存起始地址指出这个段，状态指出这个段的访问权限。

&emsp;&emsp;分段系统的动态地址转换是这样进行的：进程执行时，其段表的首地址已在基本地址寄存器中，执行的指令访问虚存（s，d）（取指令或取操作数）时，首先根据段号 s 查段表，若段已经调入内存，则得到该段的内存起始地址，然后与段内相对地址（段内偏移量 d）相加，得到实地址。如果该段尚未调入内存，则产生缺段中断，以装入所需要的段。

&emsp;&emsp;段式虚拟存储管理仍然以段式存储管理为基础，为用户提供比内存实际容量大的虚拟空间。段式虚拟存储管理把作业中的各个分段信息都保留在磁盘上，当作业可以投入执行时，做如下操作：
* 首先把当前需要的一段或几段装入内存。
* 作业执行时，如果要访问的段已经在内存，则按照“段式存储管理”中的方式进行地址转换；如果要访问的段不在内存中，则产生一个“缺段中断”，由操作系统把当前需要的段装入内存。

&emsp;&emsp;因此，在段表中应增设段是否在内存的标志以及各段在磁盘上的位置，已在内存中的段仍要指出该段在内存中的起始地址和占用内存区长度。  
&emsp;&emsp;作业执行要访问的段时，由硬件的地址转换机构查段表。若该段在内存中，则立即把逻辑地址转换成绝对地址；若该段不在内存中，则形成“缺段中断”，由操作系统处理这个中断。  
&emsp;&emsp;处理的办法是:查内存分配表，找出一个足够大的连续区以容纳该分段，如果找不到足够大的连续区则检查空闲区的总和，若空闲区总和能满足该段要求，那么进行适当移动将分散的空闲区集中；若空闲区总和不能满足该段要求，可把内存中的一段或几段调出，然后把当前要访问的段装入内存中。段被移动、调出和装入后都要对段表中的相应表目做修改。新的段被装入后应让作业重新执行被中断的指令，这时就能找到要访问的段，也可以继续执行下去。

#### 3．段页式存储管理
&emsp;&emsp;段页式管理是段式和页式两种管理方法结合的产物，综合了段式组织与页式组织的特点，根据程序模块分段，段内再分页，内存被分划成定长的页。段页式系统中虚地址形式是（段号、页号、页内偏移），如下图所示。

[![段页式系统的虚地址形式](https://s3.ax1x.com/2020/12/28/rTmG6I.png)](https://imgchr.com/i/rTmG6I)

&emsp;&emsp;系统为每个进程建立一个段表，为每个段建立一个页表。段页式管理采用段式分配、页式使用的方法，便于动态连接和存储的动态分配。这种存储管理能提高内存空间的利用率。  
&emsp;&emsp;段式虚拟管理还是以段为单位分配内存空间，整段的调出、装入，有时还要移动，这些都增加了系统的开销。如果按段页式存储管理的方式，把每一段再分成若干页面，那么，每一段不必占用连续的存储空间；甚至当内存块不够时，可只将一段中的部分页面装入内存，这种管理方式称为“段页式虚拟存储管理”。  
&emsp;&emsp;段页式虚拟存储管理为每一个装入内存的作业建立一张段表，还要为每一段建立页表。段表中指出该段的页表存放位置及长度，页表中应指出该段的各页在磁盘上的位置以及页是否在内存中，若在内存中则填上占用的内存块号。作业执行时按段号查段表，找到相应的页表再根据页号查页表，由标志位判定该页是否已在内存，若是，则进行地址转换；否则进行页面调度。地址转换过程如下图所示。

[![段页式存储地址转换过程](https://s3.ax1x.com/2020/12/28/rTmcn0.png)](https://imgchr.com/i/rTmcn0)

&emsp;&emsp;段页式虚拟存储管理结合了段式和页式的优点，但增加了设置表格（段表、页表）和查表等开销，段页式虚拟存储器一般只在大型计算机系统中使用。

### 2.2.3 设备管理
#### 1. 数据传输控制方式
&emsp;&emsp;外围设备和内存之间常用的数据传送控制方式主要有以下几种：
1. 程序控制方式  
&emsp;&emsp;处理器启动数据传输，然后等设备完成。
2. 中断方式  
&emsp;&emsp;程序控制方式不能实现并发。中断方式的数据传输过程是这样的，进程启动数据传输（如读）后，该进程放弃处理器，当数据传输完成，设备控制器产生中断请求，中断处理程序对数据传输工作处理之后，让相应进程成为就绪状态。以后，该进程就可以得到所需要的数据。
3. 直接存储访问（Direct Memory Access，DMA）方式  
&emsp;&emsp;指外部设备和内存之间开辟直接的数据交换通路。除了控制状态寄存器和数据缓冲寄存器外，DMA 控制器中还包括传输字节计数器、内存地址寄存器等。DMA 方式采用窃取（或挪用）处理器的工作周期和控制总线而实现辅助存储器和内存之间的数据交换。有的 DMA 方式也采用总线浮起方式传输大批量数据。
4. 通道方式  
&emsp;&emsp;通道又称为输入/输出处理器（Input/Output Processor，IOP），可以独立完成系统交付的输入/输出任务，通过执行自身的输入/输出专用程序（称通道程序）进行内存和外设之间的数据传输。主要有 3 种通道：字节多路通道、选择通道和成组多路通道。

&emsp;&emsp;选择和衡量控制方式的原则如下：
1. 数据传送速度足够高，能满足用户的需要但又不丢失数据。
2. 系统开销小，所需的处理控制程序少。
3. 能充分发挥硬件资源的能力，使得 I/O 设备尽量处于使用状态中，而 CPU 等待时间少


#### 2. 虚设备与SPOOLING技术
&emsp;&emsp;采用假脱机技术，可以将低速的独占设备改造成一种可共享的设备，而且一台物理设备可以对应若干台虚拟的同类设备。假脱机（Simultaneous Peripheral Operation On Line，SPOOLING）的意思是外部设备同时联机操作，又称为假脱机输入/输出操作，采用一组程序或进程模拟一台输入/输出处理器。  
&emsp;&emsp;该技术利用了专门的外围控制机将低速 I/O 设备上的数据传送到高速设备上，或者相反。但是当引入多道程序后，完全可以利用其中的一道程序来模拟脱机输入时的外围控制机的功能，把低速的 I/O 设备上的数据传送到高速磁盘上；再利用另一道程序来模拟脱机输出时外围控制机的功能，把高速磁盘上的数据传送到低速的 I/O 设备上。这样便可以在主机的控制下实现脱机输入、输出的功能。此时的外围操作与 CPU 对数据的处理同时进行。

# 第六章 开发方法
## 6.1 软件生命周期
&emsp;&emsp;有关软件生命周期的阶段划分，不同的标准有不同的规定。  
&emsp;&emsp;在 GB8566-88（《软件工程国家标准——计算机软件开发规范》）中将软件生命周期划分为 8 个阶段：`可行性研究与计划`、`需求分析`、`概要设计`、`详细设计`、`实现`、`集成测试`、`确认测试`、`使用和维护`。
1. 可行性研究与计划  
&emsp;&emsp;在决定是否开发软件之前，首先需要进行可行性研究。通过可行性研究，来确定开发此软件的必要性，并根据可行性研究的结果初步确定软件的目标、范围、风险、开发成本等内容。从而制定出初步的软件开发计划。通过可行性研究，如果确定该软件具有研发的必要，则将产生`《可行性研究报告》`和`《软件开发计划》`，并进入需求分析的阶段。
2. 需求分析  
&emsp;&emsp;需求分析是软件开发的重要阶段。经过可行性研究后，初步确定了软件开发的目标和范围，之后则需要对软件的需求进行细致的分析，来确定软件要做成什么样的。需求分析是软件开发过程中极其重要的一环，如果需求分析出现了重大偏差，那么软件开发必然会偏离正确的道路，越走越远。尤其是需求分析的错误如果在软件开发后期才被发现，修正的代价是非常大的。
3. 概要设计  
&emsp;&emsp;概要设计确定整个软件的技术蓝图，负责将需求分析的结果转化为技术层面的设计方案。在概要设计中，需要确定系统架构、各子系统间的关系、接口规约、数据库模型、编码规范等内容。概要设计的结果将作为程序员的工作指南，供程序员了解系统的内部原理，并在其基础上进行详细设计和编码工作。
4. 详细设计  
&emsp;&emsp;详细设计完成编码前最后的设计，详细设计在概要设计的基础上，进行细化，如类设计。`详细设计不是开发过程中必需的阶段`，在一些规模较小、结构简单的系统中，详细设计往往被省略。同样，在某一次软件开发中，可能只会对部分关键模块进行详细设计。
5. 实现
&emsp;&emsp;实现过程包括`编码`和`单元测试`。单元测试指的是对刚刚编写出的一个小的程序单元进行测试，如某一个过程、方法或函数。因为单元测试的对象是小的程序单元，而不是完整的程序，因此往往需要编写一些测试程序来进行测试。有效的单元测试可以大大提高编码的质量，降低软件系统的缺陷率。
6. 集成测试  
&emsp;&emsp;集成测试又称为组装测试。通过单元测试的程序并不意味着没有缺陷，当程序单元被集成到一起进行交互的时候，往往会出现单元测试中不能发现的问题。同单元测试不同，集成测试必须经过精心的组织，指定集成测试计划，确定如何将这些程序单元集成到一起，按照什么样的顺序进行测试，使用哪些测试数据等问题。
7. 确认测试  
&emsp;&emsp;当完成集成测试后，软件之间的接口方面的错误已经排除，这时需要验证软件是否同需求一致，是否达到了预期目标。同集成测试一样，确认测试也需要进行计划和组织，逐步地验证软件系统同需要的一致性。经过确认测试的软件将投入正常使用，并进入维护期。
8. 使用和维护  
&emsp;&emsp;即使通过了单元测试、集成测试和确认测试，也不可能发现软件系统中的全部缺陷；软件系统的需求也会根据业务的发展变化而变化。因此，在软件使用过程中，必须不断地对软件进行维护，修正软件中的缺陷，修改软件中已经不能适应最新情况的功能或者增加新的功能。软件维护的过程会贯穿整个软件的使用过程。当使用和维护阶段结束后，软件系统也就自然消亡，软件系统的生命周期结束。
## 6.2 软件开发模型
&emsp;&emsp;软件系统已经变得非常复杂，需要遵循一定的开发方法才能取得成功，于是称这些模式化的开发方法为开发模型。
### 6.2.1 瀑布模型
&emsp;&emsp;顾名思义，瀑布模型就如同瀑布一样，从一个特定的阶段流向下一个阶段，如下图所示。

[![瀑布模型](https://s3.ax1x.com/2020/12/31/rjKS91.png)](https://imgchr.com/i/rjKS91)
#### 1. 瀑布模型的核心思想
&emsp;&emsp;瀑布模型认为，软件开发是一个阶段化的精确的过程。就像要制造一艘航空母舰，首先需要知道航空母舰的参数（长、宽、高、排水量、航速等）。在这些参数的技术上需要对航空母舰进行设计，设计包括总体设计和详细设计。只有设计得一清二楚的图纸才能交付施工，否则造出的零件肯定拼装不到一起。制造完毕后，要把这些零件一个一个地拼装起来，拼装成发动机、船舱等部分，并检查这些部分是否符合设计标准，这就是集成测试。最后，把各个部分组合在一起，造出一艘巨大的航母。软件也同样要经过需求分析、总体设计、详细设计、编码、调试、集成测试和系统测试阶段才能够被准确地实现。在瀑布模型图中，每一阶段都有回到前一阶段的反馈线，这指的是，在软件开发中当在后续阶段发现缺陷的时候，可以把这个缺陷反馈到上一阶段进行修正。  
&emsp;&emsp;从瀑布模型图中可以看出瀑布模型的一个重要特点：软件开发的阶段划分是明确的，一个阶段到下一个阶段有明显的界线。在每个阶段结束后，都会有固定的文档或源程序流入下一阶段。在需求分析阶段结束后，需要有明确的描述软件需求的文档；总体设计结束后，需要有描述软件总体结构的文档；详细设计结束后，需要有可以用来编码的详细设计文档；而编码结束后，代码本身被作为文档流到下一个阶段。因此也称瀑布模型是`面向文档的软件开发模型`。  
&emsp;&emsp;当软件`需求明确、稳定`时，可以采用瀑布模型按部就班地开发软件，当软件需求不明确或变动剧烈时，瀑布模型中往往要到测试阶段才会暴露出需求的缺陷，造成后期修改代价太大，难以控制开发的风险。

#### 2．瀑布V模型
&emsp;&emsp;瀑布V模型是瀑布模型的一种变体。随着对瀑布模型的应用，人们发现，缺陷是无法避免的，任何一个阶段都会在软件中引入缺陷，而最后的测试也不能保证软件完全没有缺陷，只能争取在交付前发现更多的缺陷。测试成为软件开发中非常重要的环节，测试的质量直接影响到软件的质量。因此，人们对瀑布模型进行了小小的更改，提出了`更强调测试`的瀑布V模型，如下图所示。

[![瀑布V模型](https://s3.ax1x.com/2020/12/31/rjGAMR.png)](https://imgchr.com/i/rjGAMR)

整个瀑布模型在编码与调试阶段转了个弯，形成了一个对称的 V 字。瀑布 V 模型同标准瀑布模型一样，在进行完需求分析后就将进入总体设计阶段，但是除总体设计外，需求分析还有一条虚线指向系统测试。这指的是，`需求分析的结果将作为系统测试的准则`，即需求分析阶段也将产生同软件需求一致的系统测试；同时软件产品是否符合最初的需求将在系统测试阶段得到验证。以此类推，总体设计对应了集成测试，详细设计对应了单元测试。瀑布 V 模型不但保持了瀑布模型的阶段式文档驱动的特点，而且更强调了软件产品的验证工作。

### 3．瀑布模型的缺点
&emsp;&emsp;虽然是经典的开发模型，但瀑布模型中仍存在一些难以克服的缺陷，即使是在改进的瀑布 V 模型中还是会存在。  
&emsp;&emsp;首先，在瀑布模型中，需求分析阶段是一切活动的基础，设计、实现和验证活动都是从需求分析阶段的结果导出的。一旦需求分析的结果不完全正确，存在偏差，那么后续的活动只能放大这个偏差，在错误的道路上越走越远。  
&emsp;&emsp;事实上，由于用户和开发者的立场、经验、知识域都不相同，不同的人对同一件事物的表述也不同，这就造成需求分析的结果不可能精确、完整地描述整个软件系统。所以瀑布模型后期的维护工作相当繁重，而这些维护工作大多都是修正在需求分析阶段引入的缺陷。这个问题是瀑布模型难以克服的。
其次，瀑布模型难以适应变化。在瀑布模型中精确地定义了每一个阶段的活动和活动结果，而每一阶段都紧密依赖于上一阶段的结果。如果在软件的后期出现了需求的变化，整个系统又要从头开始。  
&emsp;&emsp;再次，使用瀑布模型意味着当所有阶段都结束才能最终交付软件产品，所以在提出需求后需要相当长一段时间的等待才能够看到最终结果，才能发现软件产品究竟能不能够满足客户的需求。  
&emsp;&emsp;最后，文档驱动型的瀑布模型除了制造出软件产品外还将产生一大堆的文档，大部分的文档对客户没有任何意义，但完成这些对客户没有意义的文档却需要花费大量的人力。所以瀑布模型也是一种重载过程。
### 6.2.2 演化模型
&emsp;&emsp;瀑布模型看起来很好，随着一个又一个阶段的流过，软件系统就被建立起来了。可是在应用软件开发的过程中，人们发现很难一次性完全理解用户的需求、设计出完美的架构，开发出可用的系统，这是由于人的认知本身就是一个过程，这个过程是渐进的、不断深化的。  
&emsp;&emsp;对于复杂问题，“做两次”肯定能够做得更好。那么，对于软件开发这个复杂而且与人的认知过程紧密相关的事也应该是一个渐进的过程。  
&emsp;&emsp;演化模型正是基于这个观点提出的。一般情况下，一个演化模型可以看做若干次瀑布模型的迭代，当完成一个瀑布模型后，重新进入下一个迭代周期，软件在这样的迭代过程中得以演化、完善。根据不同的迭代特点，演化模型可以演变为螺旋模型、增量模型和原型法开发。
### 6.2.3 螺旋模型
&emsp;&emsp;螺旋模型将瀑布模型和演化模型结合起来，不仅体现了两个模型的优点，而且还强调了其他模型均忽略了的风险分析。螺旋模型的每一周期都包括需求定义、风险分析、工程实现和评审 4 个阶段，由这 4 个阶段进行迭代，软件开发过程每迭代一次，软件开发就前进一个层次。采用螺旋模型的软件过程如下图所示。

[![螺旋模型](https://s3.ax1x.com/2020/12/31/rjaz6g.png)](https://imgchr.com/i/rjaz6g)

&emsp;&emsp;螺旋模型的基本做法是在“瀑布模型”的每一个开发阶段前，引入一个非常严格的风险识别、风险分析和风险控制。它把软件项目分解成一个个小项目，每个小项目都标识一个或多个主要风险，直到所有的主要风险因素都被确定。  
&emsp;&emsp;螺旋模型强调风险分析，使得开发人员和用户对每个演化层出现的风险都有所了解，继而做出应有的反应。因此，螺旋模型特别适用于`庞大而复杂`、`具有高风险`的系统，对于这些系统，风险是软件开发潜在的、不可忽视的不利因素，它可能在不同程度上损害软件开发过程，影响软件产品的质量。减小软件风险的目标是在造成危害之前，及时对风险进行识别、分析，决定采取何种对策，进而消除或减少风险的损害。  
&emsp;&emsp;与瀑布模型相比，螺旋模型支持用户需求的动态变化，为用户参与软件开发的所有关键决策提供了方便，有助于提高目标软件的适应能力，为项目管理人员及时调整管理决策提供了便利，从而降低了软件开发风险。
但是，不能说螺旋模型绝对比其他模型优越，事实上，螺旋模型也有其自身的缺点：
1. 采用螺旋模型，需要具有相当丰富的风险评估经验和专业知识。在风险较大的项目开发中，如果未能及时标识风险，势必会造成重大损失。
2. 过多的迭代次数会增加开发成本，延迟提交时间。

### 6.2.4 增量模型
&emsp;&emsp;演化模型的另一种形式是增量模型。在系统的`技术架构成熟`、`风险较低`的时候，可以采用增量的方式进行系统开发，这样可以提前进行集成测试和系统测试，缩短初始版本的发布周期，提高用户对系统的可见度。
对于增量模型，通常有两种策略。  
&emsp;&emsp;一是`增量发布`的办法。即首先做好系统的分析和设计工作，然后将系统划分为若干不同的版本，每一个版本都是一个完整的系统，后一版本以前一版本为基础进行开发，扩充前一版本的功能。在这种策略中，第一版本往往是系统的核心功能，可以满足用户最基本的需求，随着增量的发布，系统的功能逐步地丰富、完善起来。用户在很短的时间内就可以得到系统的初始版本并进行试用。试用中的问题可以很快地反馈到后续开发中，从而降低了系统的风险。在应用增量模型中需要注意：
1. 每一个版本都是一个完整的版本。虽然最初的几个增量不能完全地实现用户需求，但这些版本都是完整的、可用的。
2. 版本间的增量要均匀，这一点是很重要的。如果第一个版本花费一个月的时间，而第二个版本需要花费 6 个月的时间，这种不均匀的分配会降低增量发布的意义，需要重新调整。

&emsp;&emsp;另一种策略是原型法。原型法的每一次迭代都经过一个完整的生命周期。当用户需求很不明确或技术架构中存在很多不可知因素的时候，可以采用原型法。在初始的原型中，针对一般性的用户需求进行快速实现，并不考虑算法的合理性或系统的稳定性。这个原型的主要目的是`获得精确的用户需求`，或`验证架构的可用性`。一般情况下，会在后面的开发中抛弃这个原型，重新实现完整的系统。

### 6.2.5  构件组装模型
&emsp;&emsp;随着软构件技术的发展，人们开始尝试利用软构件进行搭积木式的开发，即构件组装模型。在构建组装模型中，当经过需求分析定义出软件功能后，将对构件的组装结构进行设计，将系统划分成一组构件的集合，明确构件之间的关系。在确定了系统构件后，则将独立完成每一个构件，这时既可以开发软件构件，也可以重用已有的构件，当然也可以购买或选用第三方的构件。构件是独立的、自包容的，因此架构的开发也是独立的，构件之间通过接口相互协作。构件组装模型的一般开发过程如下图所示。

[![构件组装模型](https://s3.ax1x.com/2020/12/31/rj0pon.png)](https://imgchr.com/i/rj0pon)

&emsp;&emsp;构件组装模型的优点如下：
1. 构件的自包容性让系统的扩展变得更加容易
2. 设计良好的构件更容易被重用，降低软件开发成本
3. 构件的粒度较整个系统更小，因此安排开发任务更加灵活，可以将开发团队分成若干组，并行地独立开发构件。

&emsp;&emsp;然而鱼与熊掌不可兼得，构件组装模型也有明显的缺点：
1. 对构件的设计需要经验丰富的架构设计师，设计不良的构件难以实现构件的优点，降低构件组装模型的重用度。
2. 在考虑软件的重用度时，往往会对其他方面做出让步，如性能等。
3. 使用构件组装应用程序时，要求程序员熟练地掌握构件，增加了研发人员的学习成本。
4. 第三方构件库的质量会最终影响到软件的质量，而第三方构件库的质量往往是开发团队难以控制的。

## 6.3 统一过程
&emsp;&emsp;统一过程（Unified Process，UP）是由 Rational 公司开发的一种迭代的软件过程，是一个优秀的软件开发模型，它提供了完整的开发过程解决方案，可以有效地降低软件开发过程的风险，经过裁剪的 UP 可以适应各种规模的团队和系统。
#### 1．UP 的二维模型
&emsp;&emsp;UP 是一个很有特色的模型，它本身是一个二维的结构，如下图所示。

[![UP模型](https://s3.ax1x.com/2020/12/31/rjBUuF.md.png)](https://imgchr.com/i/rjBUuF)

&emsp;&emsp;对于 UP 而言，时间主线就是横轴的阶段，随着时间的流逝，软件开发活动总要经过`初始`、`细化`、`构建`和`交付`这 4 个阶段方能完成。而纵轴的工作流程则描述了在不同的阶段需要进行的主要工作。例如在初始阶段，软件组织需要进行大量的调研，对软件进行业务建模、需求，同时进行一些设计以验证建模的合理性，还要进行一些实施甚至测试和部署的工作，用以验证需求和设计的工作及开发系统原型，当然配置与变更管理、项目管理和环境是在任何阶段都是不能缺少的。  
&emsp;&emsp;从这个模型中可以看出 UP 迭代的特点。任何一个阶段的工作都不是绝对的，都是相互交叠配合的。但每一个阶段都有其侧重点：  
&emsp;&emsp;在初始阶段，开发者刚刚接入系统，此时最重要的工作是界定系统范围，明确系统目的。在这一阶段，业务建模和需求工作成了重头戏。    
&emsp;&emsp;在细化阶段，开发者需要抽象出软件的逻辑模型，设计出软件的架构，在这一阶段，分析设计工作是最主要的工程活动。  
&emsp;&emsp;在构建阶段，开发者需要基本完成系统的构建，使之成为一个完整的实体，并进行测试和部署，在这一阶段，实施和测试是最主要的活动。  
&emsp;&emsp;当进入交付阶段（该阶段也经常被称为转移阶段），软件系统需求已经完全成熟或产品化，或进入下一个版本。在这一阶段不可避免地要对软件系统进行重构、修改、测试和部署。  
&emsp;&emsp;在这 4 个阶段中，各有侧重点，但也不是像瀑布模型那样完全不允许其他活动的存在。在初始阶段，为了验证开发者的想法，就需要进行一部分的实施和测试；而即使到了交付阶段，需要也可能会发生变化，仍然需要进行部分业务建模、需求和设计的活动。  
&emsp;&emsp;在每个阶段中，系统推进不是一蹴而就的。在图中将细化阶段划分为第 1 次细化和第 2 次细化，将构建阶段也划分为 3 个小阶段。在实际开发中，可以根据实际的需要划分为更多的小阶段来完成。

&emsp;&emsp;对于纵轴而言，业务建模、需求、分析设计、实施、测试、部署、配置与变更管理、项目管理、环境称为 UP 的 9 个核心工作流。可以把这 9 个工作流进行简单的分类以帮助理解，业务建模、需求、分析设计、实施、测试和部署是工程活动，而配置与变更管理、项目管理和环境是管理活动。  
&emsp;&emsp;在这 9 个工作流中，前 8 个可以说是绝大多数人都耳熟能详的东西，而“环境”工作流则相对难以理解。“环境”工作流很重要，也可以称之为“环境管理”。俗语说，“巧妇难为无米之炊”，“环境”工作流就是为软件开发准备“米”的活动。在软件开发中，需要为各种工作准备相应的工作环境，在工作环境中需要包含必需的工具、活动的指南、活动的流程规范、工作产品的模板、基本的开发设施等。在很多组织中，“环境”工作流没有得到应有的重视，或者完全被忽视，以为为开发者提供了工作台和计算机就万事大吉了，其实这种做法是错误的。每一个开发团体都有自己特定的活动准则和规范，这些准则和规范是团体协作的基础，万万少不得。没有合理的工具配备，没有充分的指南、规范和模板，软件开发的活动肯定是放羊式的管理，管理者除了一些“羊毛”外什么也收获不到。观察 UP 模型就可以发现，在每一阶段的最开始，“环境”工作流都有一个小小的波峰。在这里面，开发团队需要为开发环境进行相应的准备并在后续活动中为开发环境提供支持。

#### 2．UP的生命周期
&emsp;&emsp;UP 模型的时间主线是阶段，UP 的生命周期也是与阶段一一对应的。在 UP 的生命周期中共有 4 个里程碑：
1. 目标里程碑    
&emsp;&emsp;目标里程碑对应着先启阶段的结束，当开发者可以明确软件系统的目标和范围时即达到了该里程碑。
2. 架构里程碑    
&emsp;&emsp;架构里程碑是 UP 生命周期中的第二个里程碑，在这个里程碑前，开发者需要确定稳定的系统架构。
3. 能力里程碑    
&emsp;&emsp;当系统已经足够的稳定和成熟并完成 Alpha 测试后，认为达到了第 3 个里程碑。
4. 发布里程碑    
&emsp;&emsp;在达到发布里程碑前，需要完成系统的测试、完成系统发布和用户培训等工作。

&emsp;&emsp;在经过这 4 个里程碑后，即为一个完整的生命周期，开发出一个新的版本。此时可以关闭该产品的开发，也可以迭代进入下一版本。

#### 3．UP的特点
&emsp;&emsp;UP 是一个特点鲜明的开发模型，下面列出 UP 的一些特点：
1. UP 是一个迭代的二维开发模型，在生命周期的每一阶段都可以进行需求、设计等活动。UP 不但给出了迭代的生命周期，还给出了生命周期每一阶段的迭代指南。
2. 采用不同迭代方式的 UP 可以演变为演化模型或增量模型。
3. UP 的迭代特点使得更容易控制软件开发的风险。
4. 虽然 UP 是一个迭代的开发模型，但 UP 本身并不属于敏捷方法。相反，一般认为，未经裁减的 UP 是一个重载过程。
5. 在实际应用中可以根据具体问题对 UP 进行裁减，从而使其可以适应各种规模的软件和开发团队。

#### 4．架构设计师在UP中的活动
&emsp;&emsp;架构设计师在 UP 活动中承担着非常重要的角色。在 UP 中，架构设计师除了需要建立系统架构模型外，还需要：
1. 同需求人员和项目管理人员密切协作。
2. 细化软件架构。
3. 保持整个架构的概念完整性。具体地说，架构设计师不但需要设计系统架构，还需要定义设计方法、设计指南、编码指南、评审设计等工作。因此，有人也称 UP 是一个以架构为中心的开发模型。

## 6.4 敏捷方法
&emsp;&emsp;2001 年 2 月，在美国的犹他州，17 位“无政府主义者”共同发表了《敏捷软件开发宣言》，在宣言中指出：
>尽早地、持续地向客户交付有价值的软件对开发人员来说是最重要的。  
拥抱变化，即使在开发的后期。敏捷过程能够驾驭变化，保持客户的竞争力。  
经常交付可工作的软件，从几周到几个月，时间范围越小越好。  
在整个项目中，业务人员和开发者紧密合作。  
围绕士气高昂的团队进行开发，为团队成员提供适宜的环境，满足他们的需要，并给予足够的信任。  
在团队中，最有效率的、也是效果最好的沟通方式是面对面地交流。  
可以工作的软件是进度首要的度量方式。  
可持续地开发。投资人、开发团队和用户应该保持固定的节奏。  
不断追求优秀的技术和良好的设计有助于提高敏捷性。  
要简单，尽可能减少工作量。减少工作量的艺术是非常重要的。  
最好的架构、需求和设计都来自于一个自我组织的团队。    
团队要定期地总结如何能够更有效率，然后相应地自我调整。  

&emsp;&emsp;至此，敏捷软件联盟建立起来，敏捷软件开发方法进入了大发展的时代。这份宣言也就是敏捷方法的灯塔，所有的敏捷方法都在向这个方向努力。



<!-- 

### 1.1.3 数据表示
1. 原码  
如果机器字长为n，那么原码的定义如下：  
若X是纯整数，则当X是非负数时原码即为X，当X是负数是原码为2^(n-1) + |X|。  
若X是纯小数，则当X是非负数时原码即为X，当X是负数时原码为2^0 + |X|。  
`负数首位为1`
2. 反码
如果机器字长为n，那么反码的定义如下：  
若X是纯整数，则当X是非负数时原码即为X，当X是负数是原码为2^n - 1 + |X|。  
若X是纯小数，则当X是非负数时原码即为X，当X是负数时原码为2-2^-(n-1) + |X|。  
`负数按位取反`
3. 补码
如果机器字长为n，那么补码的定义如下：  
若X是纯整数，则当X是非负数时原码即为X，当X是负数是原码为2^n + |X|。  
若X是纯小数，则当X是非负数时原码即为X，当X是负数时原码为2 + |X|。 
`负数按位取反+1`
4. 移码
在机器字长为n，偏移量为2^(n-1)的情况下，纯整数的移码为2^(n-1)+X,纯小数的移码为1+X。`补码符号位取反即可得到移码`  

### 1.1.4 校验码
计算机通常使用校验码的方法来检测传送的数据是否出问题。  
`码距`:一个编码系统中任意两个合法编码之间至少有多少个二进制位不同  
1. 奇偶校验码  
通过在编码中增加一位校验位来使编码中1的个数为奇数（奇校验）或者偶数（偶校验），从而使码距变为2。  
常见的有：`水平奇偶校验码`，`垂直奇偶校验码`，`水平垂直校验码`
2. 海明码
海明码利用奇偶性来检错和纠错。在数据位之间的特定位置上插入K个校验位，通过扩大码距来实现检错和纠错。
3. 循环冗余校验码
利用多项式为K个数据位产生R个校验位，编码长度K+R记为N，N为CRC码的字长，又称为(n,k)码。   -->